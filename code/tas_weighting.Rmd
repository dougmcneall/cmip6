---
title: Weighting projections of tas by historical behaviour
output: html_document
---

## Weight CMIP6 model projections based on historical performance. 
Uses global mean tas from a number of CMIP6 models, with data processed by Andy Wiltshire.  
Uses the weighting scheme of [Sanderson et al. (2017)](https://www.geosci-model-dev.net/10/2379/2017/)  
This is a demonstration and exploration, so weights are arbitrary. 
Contact dougmcneall@gmail.com  
```{r}

# Load packages
library(ncdf4)
library(fields)

# r = realization index
# i = initialization index
# p = physics index
#f = forcing index
```
### Helper functions
```{r}
extractModelnames = function(filenames, startstring, stopstring){
  # Function to extract model names from a vector of cmip filenames
  # that have similar startstring and stopstring

  fs = lapply(filenames,regexpr, pattern = startstring)
  fe = lapply(filenames,regexpr, pattern = stopstring)
  out = rep(NA, length(filenames))

  for(i in 1:length(filenames)){
    out[i] = substr(filenames[i], attr(fs[i][[1]], 'match.length')+1, fe[[i]][1]-1)
  }
  out
}
```

### Use historical global tas compared with HadCRUT4.6 observations to weight the models.
Projections are using SSP585 (for maximum signal). There are all sorts of issues ignored here (for example, putting the maesurements on a common grid), but that shouldn't matter for a simple demonstration of relative weighting.

```{r}
# data directories
fn.ssp585 = dir('/data/users/hadaw/cmip6/areaavg/tas/', pattern = 'ssp585_r1i1p1')
fn.historical = dir('/data/users/hadaw/cmip6/areaavg/tas/', pattern = 'historical_r1i1p1')

# model name lists
mods.ssp585 = extractModelnames(fn.ssp585, startstring = 'tas_Amon_', stopstring = "_ssp585")
mods.historical = extractModelnames(fn.historical, startstring = 'tas_Amon_',
  stopstring = "_historical")

# Find models common to historical and future projection
common.mods = intersect(mods.ssp585, mods.historical)

nmods = length(common.mods)

fn.ssp585.kept = fn.ssp585[match(common.mods, mods.ssp585)]
fn.historical.kept = fn.historical[ match(common.mods, mods.historical)]

# useful
# https://stackoverflow.com/questions/17215789/extract-a-substring-in-r-according-to-a-pattern
# start with strsplit, pmatch, charmatch
```

### Load and process historical model data
Uses a time series of the last 30 years of the historical runs.

```{r fig.height = 8, fig.width = 8}


# Historical years
yrs = 1985:2014
nyr = length(yrs)

#variable matrix (turn this into a function?
varmat = matrix(nrow = length(fn.historical.kept), ncol = nyr)

for(i in 1:length(fn.historical.kept)){
# Open and extract data from the netcdf file
  fn = fn.historical.kept[i]
  fnpath = paste0("/data/users/hadaw/cmip6/areaavg/tas/", fn)

  nc = nc_open(fnpath)

  v = ncvar_get(nc, 'tas')
  nc_close(nc)
  
  vtail = tail(v, nyr)
  varmat[i, ] = vtail
}


par(las = 1)
matplot(yrs, t(varmat), 
        type = 'l', lty = 'solid', main = 'raw model tas', ylab = 'tas (K)')

# Amomalize model runs to the mean of each run
varmat.means = apply(varmat, 1, mean)

# varmat.anom is the variable timeseries matrix anomaly
varmat.anom = sweep(varmat, 1, STATS = varmat.means)
```
### Load Observations
Currently uses HadCRUT4.6

```{r fig.width = 7, fig.height = 7}


# Load up the HadCRUT4 data
obsmat = read.table(file = 'https://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/time_series/HadCRUT.4.6.0.0.annual_ns_avg.txt')

obsyears = obsmat[,1]
obsmed   = obsmat[,2]

obs.ix = which(obsyears %in% yrs)

obs = obsmed[obs.ix]
obs.anom = obs - mean(obs)


# Matrix of tas errors (differences between the anomalies of the obs and models)
tas.err = sweep(varmat.anom, 2, obs.anom)

# RMSE for tas, the main measure for both skill and similarity
tas.rmse = sqrt(apply( (tas.err)^2, 1, mean))

best.ix = which.min(tas.rmse) # 
common.mods[best.ix]
min.rmse = min(tas.rmse)
min.rmse

# Distance from observations for each of the models.
dotchart(tas.rmse, labels = common.mods, main = 'TAS RMSE by model')

rbPal <- colorRampPalette(c('darkblue', 'lightgrey'))
linecols = rbPal(10)[as.numeric(cut(tas.rmse,breaks = 8))]

par(las = 1)
matplot(yrs, t(varmat.anom), type = 'l', 
        lty = 'solid',
       main = "Model temperature anomalies, weighted by rmse",
       ylab = 'tas anomaly (K)', col = linecols
)
lines(yrs, obs.anom, col = 'red', lty = 'solid')
legend('topleft', lty = 'solid', col = 'red', legend = 'observations', bty = 'n')

```

### Weighting section

```{r fig.width = 6, fig.height = 7}

get_w_q = function(rmse, D_q){ 
  # Skill weighting function
  out = exp(-(rmse/D_q)^2)
  out
}

D_q = 0.1 # guess for now

w_q = get_w_q(rmse = tas.rmse, D_q = D_q)

w_q

plot(tas.rmse, w_q, xlab = 'RMSE', ylab = 'Performance weight', pty = 'n',
     ylim = c(0,1),
     xlim = c(min(tas.rmse), (max(tas.rmse)*1.05))
     )

# A quick look at how D_q affects the skill weighting.
# A lower value (0.1 for example) brings the weighting of some models
# close to zero, and has a larger impact on the final weighting.
D_q.vec = seq(from = 0.1, to = 0.9, by = 0.05)

for(i in 1:length(D_q.vec)){

  w_q_i = get_w_q(rmse = tas.rmse, D_q = D_q.vec[i])
  tas.rmse.sort = sort(tas.rmse, index.return = TRUE)
  w_q_i.sort = w_q_i[tas.rmse.sort$ix]
  
  points(tas.rmse.sort$x, w_q_i.sort, type = 'o')

  text(
       tail(tas.rmse.sort$x,1),
       tail(w_q_i.sort,1),
       labels = D_q.vec[i],
       pos = 4,
       col = 'red'
       )
}
legend('topright', legend = 'D_q', text.col = 'red', bty = 'n')

```


### Similarity weighting
(Not including the observations at the moment)


```{r  fig.width = 10, fig.height = 10}

# measure pairwise distance between models
d_ij = dist(varmat.anom, upper = TRUE, diag = TRUE)
d_ij.matrix = as.matrix(d_ij)

d_ij.formatted = apply(d_ij.matrix, 2, rev)

# I think this is the right way round.
par(mar = c(9,9,3,3))
image.plot(1:nmods, 1:nmods,t(d_ij.formatted), axes = FALSE, xlab = '', ylab = '')
axis(2, at = nmods:1, labels = common.mods, las = 1)
axis(1, at = 1:nmods, labels = common.mods, las = 2)

# D_u is the radius of similarity
D_u = 0.5#* min.rmse

# S is a similarity score
S = exp( - (d_ij.matrix / D_u)^2)

# R_u is a model's repetition
R = rep(NA, nrow(S))

for(i in 1:nrow(S)){
  r = sum(S[i,][-i])
  R[i] = r
}

R_u = 1 + R

# w_u is the Similarity weighting
w_u = 1/R_u

w_u
```


### Final combination of the performance and similarity weights

```{r  fig.width = 5, fig.height = 8}

w.raw = w_q * w_u

# calculate A, a constant that ensures A sums to one
Ainv = sum(w.raw)
A = 1/Ainv

# final weights
w = w.raw*A

# I think that the independence weighting is dominating.
# Check how rmse and weighting interact

#dev.new(width = 5, height = 8)
#options(repr.plot.width = 5, repr.plot.height = 8)
plot(tas.rmse, w_q, xlab = 'RMSE', ylab = 'weight', pty = 'n',
     ylim = c(0,1),
     xlim = c(min(tas.rmse), (max(tas.rmse)*1.05)),
     pch = 19
     )
points(tas.rmse, w_u, col = 'red', pch = 19)

points(tas.rmse, w.raw, col = 'orange', pch = 19)
points(tas.rmse, w, col = 'blue', pch = 19)
legend('topleft', pch = 19, legend = c('w_q','w_u','w.raw', 'w') , col = c('black', 'red','orange', 'blue'))

```
Apply a simple weighting by sampling from the distribution of historical warming, weighted according to w
```{r}

# create index by which we sample the model matrix
n = 10000
ix.w = sample(1:nmods, n, replace = TRUE, prob = w) # weighted sample
ix = sample(1:nmods, n, replace = TRUE)             # unweighted sample

ts.sample.weighted = varmat.anom[ix.w, ]
ts.sample.unweighted = varmat.anom[ix, ]

mat.change = function(X){
  cols = ncol(X)
  out = X[, cols] - X[,1]
  out
}
warming.weighted = mat.change(ts.sample.weighted)
warming.unweighted = mat.change(ts.sample.unweighted)

#dev.new()
par(mfrow = c(2,1))
hist(warming.unweighted, xlim = c(0,1.5), main = 'unweighted historical (resampled)' )
hist(warming.weighted, xlim = c(0,1.5 ), main = 'weighted historical (resampled)')
```
### Mean warming over the unweighted and weighted sample
```{r}
paste0('mean warming (unweighted) = ', mean(warming.unweighted))
paste0('mean warming (weighted) = ', mean(warming.weighted))

```
## Extract future projection data
Start with a single realisation from each model (r1i1p1), to keep everything the same.  
Hadley models use f2 - they don't have f1
```{r fig.width = 7, fig.height = 7}

fyrs = 2015:2099
fnyr = length(fyrs)
futuremat = matrix(nrow = length(fn.ssp585.kept), ncol = fnyr)

for(i in 1:length(fn.ssp585.kept)){
# Open and extract data from the netcdf file
  fn = fn.ssp585.kept[i]
  fnpath = paste0("/data/users/hadaw/cmip6/areaavg/tas/", fn)

  nc = nc_open(fnpath)

  v = ncvar_get(nc, 'tas')
  #print(length(v))
  nc_close(nc)
  
  vhead = head(v, fnyr)
 
  futuremat[i, ] = vhead
}


matplot(fyrs,t(futuremat), type = 'l', lty = 'solid', main = 'raw model projection data')

# Amomalize to the mean
# varmat.anom is the variable timeseries matrix anomaly
futuremat.changemat = sweep(futuremat, 1, STATS = futuremat[,1])
future.diff = futuremat.changemat [,ncol(futuremat.changemat )]

matplot(fyrs, t(futuremat.changemat), type = 'l', lty = 'solid', main = 'anomalized model projection data')


```

### Weight the future projections of tas change

```{r}
future.warming = sample(future.diff , n, replace = TRUE)
future.warming.weighted = sample(future.diff, n, replace = TRUE, prob = w)


#dev.new()
par(mfrow = c(2,1))
hist(future.warming, xlim = c(0,10), main = 'Unweighted future warming')
hist(future.warming.weighted, xlim  = c(0,10), main = 'weighted future warming')


paste0('mean future warming unweighted = ', mean(future.warming))
paste0('mean future warming weighted = ', mean(future.warming.weighted))
```
A better bootsrapping technique is to create a large number of  samples, each the same size as the models. This gives you an estimate of the mean (and the sampling uncertainty of that mean).
```{r}

mfu  = rep(NA,n)
mfw =  rep(NA,n)

sdfu  = rep(NA,n)
sdfw =  rep(NA,n)

for(i in 1:n){

    sample.future.unweighted = sample(future.diff, nmods, replace = TRUE)
    sample.future.weighted = sample(future.diff, nmods, replace = TRUE, prob = w)
    
    mfu[i] = mean(sample.future.unweighted )
    mfw[i] = mean(sample.future.weighted)
    
    sdfu[i] = sd(sample.future.unweighted )
    sdfw[i] = sd(sample.future.weighted) 
}

# mean of all the means of the bootstap sample
paste0( 'mean means unweighted = ', mean(mfu))
paste0( 'mean means weighted = ', mean(mfw))

# mean of all the standard deviations of the bootstrap samples
paste0( 'mean sds unweighted = ', mean(sdfu))
paste0( 'mean sds weighted = ', mean(sdfw))

```

