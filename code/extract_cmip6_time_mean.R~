# Write time series matrix of cmip6 annual averages.
# Data is initially processed by Andy Wiltshire.
# dougmcneall@gmail.com


library(ncdf4)

# The tricky part is creating a list of all the relevant files
# in the directory.

# Example code

#fnvec = c("Annual.cs_gb.global_sum.txt",
#          "Annual.cv.global_sum.txt",
#          "Annual.gpp_gb.global_sum.txt",
#          "Annual.nbp.global_sum.txt",
#          "Annual.npp_n_gb.global_sum.txt",
#          "Annual.runoff.global_sum.txt")

## fnallvec = dir('data/ES_PPE_ii_test/', pattern = 'Annual')
## # WARNING - hard coded hack to sort
## fidx = grep("Annual.(?!Amazon).*", fnallvec, perl=TRUE)
## fnvec_interim = fnallvec[fidx]
## fidx2 = grep("sum.(?!standard).*", fnvec_interim, perl=TRUE)
## fnvec = fnvec_interim[fidx2]
## fnlocvec = paste0('data/ES_PPE_ii_test/', fnvec)

## # Extract the bit of the filename that describes the data
## fs = lapply(fnvec,regexpr, pattern = 'Annual')
## fe = lapply(fnvec,regexpr, pattern = 'global_sum.txt')

## fnams = rep(NA, length(fnvec))
## for(i in 1:length(fnvec)){
##   fnams[i] = substr(fnvec[i], attr(fs[[1]], 'match.length')+2, fe[[i]][1]-2)
## }

# r = realization index
# i = initialization index
# p = physics index
#f = forcing index


# start with those models that are r1i1p1f1, to keep everything the same.
fnallvec = dir('/data/users/hadaw/cmip6/areaavg/tas/', pattern = 'historical_r1i1p1f1')

yrs = 1985:2014
nyr = length(yrs)
varmat = matrix(nrow = length(fnallvec), ncol = nyr)

for(i in 1:length(fnallvec)){
# Open and extract data from the netcdf file
  fn = fnallvec[i]
  fnpath = paste0("/data/users/hadaw/cmip6/areaavg/tas/", fn)

  nc = nc_open(fnpath)

  v = ncvar_get(nc, 'tas')
  nc_close(nc)
  
  vtail = tail(v, nyr)
 
  varmat[i, ] = vtail
}

  
matplot(t(varmat), type = 'l')

# Amomalize to the mean
varmat.means = apply(varmat, 1, mean)
varmat.anom = sweep(varmat, 1, STATS = varmat.means)

# Load up the HadCRUT4 data
obsmat = read.table(file = 'https://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/time_series/HadCRUT.4.6.0.0.annual_ns_avg.txt')

obsyears = obsmat[,1]
obsmed   = obsmat[,2]

obs.ix = which(obsyears %in% 1985:2014)

obs = obsmed[obs.ix]
obs.anom = obs - mean(obs)


matplot(yrs, t(varmat.anom), type = 'l', lty = 'solid', col = 'grey')

lines(yrs, obs.anom, col = 'red', lty = 'solid')

# Matrix of tas errors (differences between the anomalies of the obs and models)
tas.err = sweep(varmat.anom, 2, obs.anom)

matplot(yrs, t(tas.err), type = 'l', lty = 'solid', col = 'grey')

tas.rmse = sqrt(apply( (tas.err)^2, 1, mean))

# -------------------------------------------------------------------
# Weighting section from Sanderson et al. (2017)
# -------------------------------------------------------------------

# Skill weighting

D_q = 0.5 # guess for now

w_q = exp(  - (tas.rmse/D_q)^2)


# Similarity weighting
# (Not including the models at the moment)

# measure pairwise distance between models
d_ij = dist(varmat.anom, upper = TRUE, diag = TRUE)
d_ij.matrix = as.matrix(d_ij)

# D_u is the radius of similarity
D_u = 0.5

# S is a similarity score
S = exp( - (d_ij.matrix / D_u)^2)

# R_u is a model's repetition

R = rep(NA, nrow(S))

for(i in 1:nrow(S)){
  r = sum(S[i,][-i])
  R[i] = r
}

R_u = 1 + R

# w_u is the Similarity weighting
w_u = 1/R_u

w.raw = w_q * w_u

# calculate A, a constant that ensures A sums to one
Ainv = sum(w.raw)
A = 1/Ainv


# final weights
w = w.raw*A














